# Attentive Mimicking:Better Word Embeddings by Attending to Informative Contexts
在稀疏上下文中低频单词的嵌入的质量会大大降低，模仿被认为是一种可行的解决方案。通过给定标准算法的词嵌入，再计算
低频的词嵌入，在该文中，作者引入了注意力模仿模型，该模型不仅仅能够可以体现单词的表面形式，同时还能使用词汇的所有上下文
的最有用和最可靠的信息来计算词嵌入。
## 背景
词嵌入在nlp中取得巨大的性能提升，从传统的单词表面信息到统计学表示都不能很好的体现单词的全文信息，到最近以来的词嵌入的工作，通过嵌入将上下文的信息嵌入到单词的表现形式中去
使得单词的信息更加丰富。然而词嵌入通常需要对单词进行多次观察才能学到更好的表现形式。克服这种限制并改进低频单词的嵌入的一个方法是将表层信息添加到学习范围中去。
本文中讨论的关键在于，通常一个词的上下文中只有很少一部分能提供关于其含义的有价值的信息。然而，当前的技术水平将所有的上下文视为相同作用。作者通过引入一种更智能的机制来解决
这个问题：作者不是使用所有的上下文，而是通过关注来选择一个信息更为丰富的上下文的子集进行学习。这个机制基于以下观察：在许多情况下，给定单词的可靠上下文往往彼此相似。作者将
此结构称为注意力模仿(AM)。
### 本文的贡献：
* 介绍了注意力模仿模型。通过关注最有用的上下文，它可以为低频和中频单词生成高质量的嵌入。
* 提出了一种基于VecMap的新式评估方法，通过这种方式能够轻松评估低频和中频单词的嵌入质量。
* 通过注意力的模仿可以改善各种数据集上的单词嵌入性能。
## 注意力模仿
### 上下文模型
上下文模型（FAM）中需要一个维度为d的嵌入空间将高质量的v∈Rd分配给高频单词，给定一个低频或者
新单词w以及该词出现的一组上下文c，FCM模型可以在给定的向量空间中为单词w计算出一个合适的嵌入v[w,c]，
这是通过计算出两个不同的嵌入实现的，其中一种嵌入仅使用表面形态信息，而另一个则是使用了上下文信息
。表面形态嵌入是通过对模型学习的一组n-gram嵌入进行平均而获得的；上下文嵌入是通过对c上下文中单词
的所有嵌入求平均值而获得的。然后使用权重系数α和大小为d × d的矩阵A组合这两个嵌入，从而形成形态上
下文嵌入。  
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM1.png)   
上式中系数α是两个嵌入的洗漱，可通过下式得到：  
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM2.png)   
其中u∈R2d,b∈R是可学习参数，σ表示为sigmoid函数。
### 上下文注意力机制
FCM同样关注一个词的上下文，但是在实际情况中只有很少的一部分上下文对推断某个单词的含义有具体的作用
论文中作者通过引入注意力模仿(AM)很好的解决这个问题，通过一个可靠性来描述上下文的重要性，通过上下文的可靠性
来为上下文分配不同的权重，这里令C={C1,C2,...,Cm}，其中每个Ci是一个单词组，这里将FCM的上下文嵌入替换为加权嵌入。  
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM3.png)   
其中Vci是Ci中单词嵌入的平均值，α是上下文的可靠性的参数，为了使α可靠，这里衡量的关键在于可靠的上下文通常
于其他的上下文表达是一致的。  
这里入假如一个单词w,加入其中w的60%的上下文是于银行相关的，这里由于高度的相关上下文的佐证，所以可以很合理的假设这些上下文于w来自同一个域。
同时于银行无关的那40%的上下文可以认为信息较少，这里定义两个上下文的相似度的定义为：  
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM4.png)  
其中M是一个Rd*d的矩阵，它是一个可学习参数，同时上下文可靠性的定义为：  
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM5.png)  
这里Z是一个归一化参数，确保权值之和为1，该模型通过从大型语料库中随机抽取单词w和上下文C来训练模型并模仿w的原始嵌入，即是最
小化原始嵌入和v(w,C)的平方距离。
## 实验
在实验中，通过使用wiki的语料库(WWC)来训练所有的嵌入模型，为了获取FCM和AM的训练实例(w,c)，根据
WWC的频率对单词和上下文进行采样，采样的是频率至少出现100次的单词，使用skipgram训练FCM和AM。
试验中有两个方面和Schickand Schutze(2019)不同  
* 没有使用固定数量的上下文表示C，而是随机采样1到64个上下文
* 将训练轮数固定为5轮
### Vecmap
论文中提出了一种新的评估方法，该方法通过将WWC的常用词降采样为固定的出现次数来显示的评估低中频的词嵌入。
然后，作者将从原始语料库获得的skipgram嵌入与通过在降采样语料库上训练的某种模型学习的嵌入进行比较。使用
VecMap将两个嵌入空间转换为一个公共空间, 通过提供除降采样词外的所有词作为映射字典。从直觉上讲，模型从少
量观察值推断嵌入的效果越好，其嵌入与该公共空间中的嵌入的相似性就越高。因此，作者通过计算模型嵌入和skipgram嵌入
之间的平均余弦相似度来衡量模型的质量。作为基线, 在缩小样本的语料库上训练skipgram和fastText。然后
在skipgram上训练Mimick、FCM和AM。
## 实验结果
### Vecmap
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM6.png)  
### 情感词典
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM7.png)  
### 实体输入
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM8.png) 
### Chimeras
![word2vec](https://github.com/opprash/braveRL/blob/master/datas/AM9.png) 