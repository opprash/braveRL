# 深度神经网络模型压缩和加速的方法综述
论文： A Survey of Model Compression and Acceleration for Deep Neural Networks  　　
链接： https://www.paperweekly.site/papers/1675  　　
作者： Yu Cheng / Duo Wang / Pan Zhou / Tao Zhang  　　
## 研究背景
深度学习的普及带来的是模型准确率的明显提升，但是在模型训练中由于使用了很多的神经网络导致网络参数的极具增加使得
训练的时间成本和机器硬件性能的要求大大提升，有的时候在效率和投入之间的预算甚至会大打只扣，可以想象一个数百甚至十亿级别的参数对训练的要求是一个怎样的影响。
尽管现代的运算处理技术的大大提升，如GPU,TPU等硬件在矩阵运算这里能够大大加速模型的训练过程但是这也只是一种解决训练平台并没有完全解决模型需要
解决的问题，那就是训练本身，假设我们能够将模型的参数进行一定的压缩，即使牺牲一点点精确度也是值得的。  
Krizhevsky在2014年的一篇文章中就曾经提出两大观点：卷积层占据了大约 90-95% 的计算时间和参数规模，有较大的值；全连接层占据了大约 5-10% 的计算
时间，95% 的参数规模，并且值较小。这为后来的研究深度模型的压缩与加速提供了统计依据。
一个典型的例子是具有 50 个卷积层的 ResNet-50 需要超过 95MB 的存储器以及 38 亿次浮点运算。在丢弃了一些冗余的权重后，网络仍照常工作，但节省了超过75％
的参数和 50％ 的计算时间。
## 现状
* 参数修减和共享
* 低秩因子分解
* 转移/紧凑卷积滤波器
* 知识蒸馏
## 综述
参数修剪和共享针对模型参数的冗余性，这里它试图去除冗余和不重要的项，基于低秩因子分解的技术使用矩阵/张量分解来估计深度学习模型的信息参数
转移/紧凑卷积滤波器设计了特殊结构的卷积滤波器来降低存储和计算复杂度。知识蒸馏方法通过学习一个蒸馏模型来训练一个更紧凑的网络。  
一般来说，参数修剪和共享，低秩分解和知识蒸馏可用在全连接层和卷积层，转移/紧凑卷积滤波器仅仅支持卷积层。  
低秩因子分解和转移/紧凑滤波器提供了一个端到端的流水线，使得他们很容易在CPU或者GPU上面实现。  
参数修剪和共享使用不同的方法如矢量量化，二进制编码和稀疏约束来执行任务。  
![m_c1](https://github.com/opprash/braveRL/blob/master/datas/m_c1.png)  
### 参数修剪和共享
**减少冗余（信息冗余和参数空间冗余）的方式，这些参数修剪和共享可进一步分为三类：**模型量化和二进制化、参数共享和结构化矩阵。
#### 量化和二进制化
![m_c2](https://github.com/opprash/braveRL/blob/master/datas/m_c2.png)  
网络量化通过减少表示每个权重所需的比特数来压缩原始网络。Gong et al. 对参数值使用 K-Means 量化。Vanhoucke et al. 使用了8比特
参数量化可以在准确率损失极小的同时实现大幅加速。
Han Ｓ**提出了一整套的深度网络的压缩流程**：首先修剪不需要的链接，重新训练稀疏链接的网络。然后使用权重共享量化连接的权重，在对量化后的
权重和码本进行霍夫曼编码，以进一步降低压缩率。如图２所示，**包含了三阶段的压缩方法：修剪、量化（quantization）和霍夫曼编码。**  
**缺点**：在量化比较多的情况下准确率能够保持较好，但是对于二值量化网络的准确率在处理大型CNN网络，如googleNet时会大大降低，另一个就是现有的二进制方法都
基于简单的矩阵相似，忽视了二进制化对准确率损失的影响。
### 剪枝和共享
网络的剪枝和共享最原始是用于解决过拟合问题，如决策树的剪枝就是为了防止过拟合的潜在问题，但是目前这些方法更多是用于降低网络的复杂度。　
早期的剪枝方法称为偏差权重衰减，其中最优脑损伤和最优脑手术方法，是基于损失函数的Hessian矩阵来减少链接的数量。
他们的研究表明这种剪枝方法的精确度比基于重要性的剪枝方法更高，在最近的发展中一个方向是在预先训练的CNN模型中修剪冗余的、非信息量的权重。　　
在矩阵很稀疏的情况下培训紧凑的CNN也变得特别流行，这些稀疏约束通常作为$L_0$或$L_1$范式调节器在优问题中引入。　　
**缺点**：首先，若使用了$L_0$或$L_1$正则化，剪枝方法需要更多次的迭代才能收敛，其次，所有的剪枝方法都需要手动设置层的超参数，这会导致这种方法在
许多应用中变得及其复杂。　　
![m_c3](https://github.com/opprash/braveRL/blob/master/datas/m_c3.png)  
### 设计结构化矩阵
该方法中假设一个m*n的矩阵只需要m*n个参数来描述，就是一个结构化的矩阵，通常这样的结构不仅能减少内存还能通过快速的矩阵－向量乘法和
梯度计算来提升训练的速度。  
　
缺点：这种方法存在一个潜在的问题就是结构约束会导致精确度的丢失，因为约束可能会给模型带来片餐，你可以通过一种合理的假设来验证种说法，比如说
从一个网络层到下一个网络层的参数的个数往往是不确定的，在不同的情况下参数数量肯定是不同的，这个时候你如果使用一个具体的框架来
形容他，不管怎么说也只能说是相对合理不能说绝对合理。另外，如何找到一个合适的m*n的矩阵也是及其困难的。
### 低秩分解和稀疏性
一个典型的CNN卷积核是一个4D张量，而全连接层也可以当成一个2D矩阵，低秩分解同样可行。这些张量中可能存在大量的冗余。所有近似过程都是逐层
进行的，在一个层经过低秩滤波器近似之后，该层的参数就被固定了，而之前的层已经用一种重构误差标准（reconstruction error criterion）微调过。
这是压缩2D卷积层的典型低秩方法，如图4所示。  
![m_c4](https://github.com/opprash/braveRL/blob/master/datas/m_c4.png)   
使用低阶滤波器加速卷积的时间已经很长了，例如，高维DCT离散余弦变换）和使用张量积的小波系统分别由1D DCT变换和1D小波构成。  
学习可分离的1D滤波器由Rigamonti等人提出，遵循字典学习的想法。Jaderberg的工作提出了使用不同的张量分解方案，在文本识别
准确率下降 1％ 的情况下实现了4.5倍加速。  
一种flatten结构将原始三维卷积转换为3个一维卷积，参数复杂度由O(XYC)降低到O(X+Y+C)，运算复杂度由O(mnCXY)降低到O(mn(X+Y+C)。  
低阶逼近是逐层完成的。完成一层的参数确定后，根据重建误差准则对上述层进行微调。这些是压缩二维卷积层的典型低秩方法，如图2所示。  
**缺点**：低秩方法很适合模型压缩和加速，但是低秩方法的实现并不容易，因为它涉及计算成本高昂的分解操作。另一个问题是目前的方法都是
逐层执行低秩近似，无法执行全局参数压缩，因为不同的层具备不同的信息。最后，分解需要大量的重新训练来达到收敛。　　
### 迁移/压缩卷积滤波器　
虽然目前缺乏强有力的理论，但大量的实证证据支持平移不变性和卷积权重共享对于良好预测性能的重要性。  
使用迁移卷积层对CNN模型进行压缩受到Cohen的等变群论（equivariant group theory）的启发。使x作为输入，Φ(·)作为网络或层，T(·)作为变换矩阵。
则等变概念可以定义为：
$$T^'Φ(x)=Φ(Tx)$$
即使用变换矩阵T(·)转换输入x，然后将其传送至网络或层Φ(·)，其结果和先将x映射到网络再变换映射后的表征结果一致。注意T和T'在作用到不同对象时可能
会有不同的操作。根据这个理论，将变换应用到层次或滤波器 Φ(·)来压缩整个网络模型是合理的。   
使用紧凑的卷积滤波器可以直接降低计算成本。在Inception结构中使用了将3*3卷积分解成两个1*1的卷积；SqueezeNet提出用１*1卷积来代替3*3卷积，
与AlexNet相比，SqueezeNet创建了一个紧凑的神经网络，参数少了50倍，准确度相当。   
**缺点**：这种方法仍有一些小问题解决。首先，这些方法擅长处理广泛/平坦的体系结构（如VGGNet）网络，而不是狭窄的/特殊的（如GoogleNet，ResidualNet）。
其次，转移的假设有时过于强大，不足以指导算法，导致某些数据集的结果不稳定。
### 知识蒸馏
利用知识转移（knowledge transfer）来压缩模型最早是由Caruana等人提出的。他们训练了带有伪数据标记的强分类器的压缩/集成模型，并复制了原始大型
网络的输出，但是，这项工作仅限于浅模型。  
后来改进为知识蒸馏，将深度和宽度的网络压缩成较浅的网络，其中压缩模型模拟复杂模型所学习的功能，主要思想是通过学习通过 softmax 获得的类分布输出，
将知识从一个大的模型转移到一个小的模型。   
Hinton的工作引入了知识蒸馏压缩框架，即通过遵循“学生-教师”的范式减少深度网络的训练量，这种“学生-教师”的范式，即通过软化“教师”的输出而惩罚“学生”。
为了完成这一点，学生学要训练以预测教师的输出，即真实的分类标签。这种方法十分简单，但它同样在各种图像分类任务中表现出较好的结果。  
基于知识蒸馏的方法能令更深的模型变得更加浅而显著地降低计算成本。但是也有一些缺点，例如只能用于具有 Softmax 损失函数分类任务，这阻碍了其应用。
另一个缺点是模型的假设有时太严格，其性能有时比不上其它方法。  
